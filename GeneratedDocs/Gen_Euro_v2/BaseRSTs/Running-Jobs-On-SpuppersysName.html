

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>5 - Running Jobs on SRuppersysName &mdash; UTDallas HPC Cluster Users Guide 1.2 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> UTDallas HPC Cluster Users Guide
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../IO-Training-v1.2.html">UTDallas HPC Cluster Users Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../IO-Training-v1.2.html#acceptable-user-guidelines">Acceptable User Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../IO-Training-v1.2.html#introduction">Introduction</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../Connecting-To-IO.html">1 - Connecting to Io</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Connecting-To-IO.html#microsoft-windows">1.1 - Microsoft Windows</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Connecting-To-IO.html#putty">1.1.1 - PuTTY:</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Connecting-To-IO.html#bitvise">1.1.2 - Bitvise:</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Connecting-To-IO.html#mobaxterm">1.1.3 - MobaXTerm:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Connecting-To-IO.html#for-mac-users">1.2 - For Mac Users</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Connecting-To-IO.html#for-linux-users">1.3 - For Linux Users</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Connecting-To-IO.html#mailing-list">1.4 - Mailing List</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../Moving-Around-IO.html">2 - Moving around Io</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Moving-Around-IO.html#linux-commands">2.1 - Linux Commands</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Moving-Around-IO.html#moving-around">2.1.1 - Moving Around</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Moving-Around-IO.html#making-files-and-directories">2.1.2 - Making Files and Directories</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Moving-Around-IO.html#moving-files-and-directories">2.1.3 - Moving Files and Directories</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Moving-Around-IO.html#removing-files-and-directories">2.1.4 - Removing Files and Directories</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Moving-Around-IO.html#viewing-and-editing-files">2.1.5 - Viewing and Editing Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Moving-Around-IO.html#automating-commands">2.1.6 - Automating Commands</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Moving-Around-IO.html#systemname-specific-instructions-and-programs">2.2 - Io Specific Instructions and Programs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Moving-Around-IO.html#environmental-variables">2.2.1 - Environmental Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Moving-Around-IO.html#custom-programs">2.2.2 - Custom Programs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../IO-Space-Constraints.html">3 - Io Space Constraints</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../IO-Space-Constraints.html#types-of-space">3.1 - Types of Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../IO-Space-Constraints.html#space-policies">3.2 - Space Policies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../IO-Space-Constraints.html#checking-available-space">3.3 - Checking Available Space</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../IO-Compilers-And-Modules.html">4 - Io Compilers and Modules</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../IO-Compilers-And-Modules.html#modules">4.1 - Modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../IO-Compilers-And-Modules.html#displaying-loaded-modules">4.1.1 - Displaying Loaded Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../IO-Compilers-And-Modules.html#listing-available-modules">4.1.2 - Listing Available Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../IO-Compilers-And-Modules.html#adding-modules-to-a-user-s-account">4.1.3 - Adding Modules to a User’s Account</a></li>
<li class="toctree-l3"><a class="reference internal" href="../IO-Compilers-And-Modules.html#removing-modules-from-your-account">4.1.4 - Removing Modules From Your Account</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../IO-Compilers-And-Modules.html#compilers">4.2 - Compilers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../IO-Compilers-And-Modules.html#list-available-compilers-and-mpi-stacks">4.2.1 - List Available Compilers and MPI Stacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../IO-Compilers-And-Modules.html#changing-currently-loaded-compilers-or-mpi-stacks">4.2.2 - Changing Currently Loaded Compilers or MPI Stacks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../Running-Jobs-On-IO.html">5 - Running Jobs on Io</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Running-Jobs-On-IO.html#preparing-to-queue-a-task">5.1 - Preparing to Queue a Task</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Running-Jobs-On-IO.html#queuing-a-task">5.2 - Queuing a Task</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Running-Jobs-On-IO.html#serial-task">5.2.1 - Serial Task</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Running-Jobs-On-IO.html#parallel-tasks">5.2.2 - Parallel Tasks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Running-Jobs-On-IO.html#checking-on-a-running-task">5.3 - Checking on a Running Task</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Running-Jobs-On-IO.html#debugging-mpi-with-slurm-and-gdb">5.4 - Debugging MPI with Slurm and gdb</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Running-Jobs-On-IO.html#running-interactive-jobs">5.5 - Running Interactive Jobs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Running-Jobs-On-IO.html#running-one-time-instance-jobs">5.5.1 - Running One-Time Instance Jobs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Running-Jobs-On-IO.html#running-persistent-jobs">5.5.2 - Running Persistent Jobs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Running-Jobs-On-IO.html#logging-into-a-particular-node">5.5.3 - Logging into a Particular Node</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Running-Jobs-On-IO.html#viewing-results">5.6 - Viewing Results</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../IO-Appendicies.html">Appendices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../IO-Appendicies.html#appendix-a-linux-commands">Appendix A - Linux Commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="../IO-Appendicies.html#appendix-b-slurm-commands">Appendix B - Slurm Commands</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">UTDallas HPC Cluster Users Guide</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>5 - Running Jobs on SRuppersysName</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/BaseRSTs/Running-Jobs-On-SpuppersysName.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="running-jobs-on-systemname">
<h1>5 - Running Jobs on SRuppersysName<a class="headerlink" href="#running-jobs-on-systemname" title="Permalink to this headline">¶</a></h1>
<p><strong>What is Slurm</strong></p>
<blockquote>
<div><p>Slurm is an open source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters. Slurm requires no kernel modifications for it’s operation and is relatively self-contained. Slurm has a centralized manager to monitor resources and work. There may also be a backup manager to assume those responsibilities in the event of failure.</p>
</div></blockquote>
<div class="section" id="preparing-to-queue-a-task">
<h2>5.1 - Preparing to Queue a Task<a class="headerlink" href="#preparing-to-queue-a-task" title="Permalink to this headline">¶</a></h2>
<p>Before the user can queues a task, the user should check and see the status of cluster.  This is done by running the command <code class="docutils literal notranslate"><span class="pre">sinfo</span></code>. By doing this, the user can see what resources are available.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid@CBsysname ~]$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
debug        up    2:00:00      2   idle compute-6-9-[0-1]
normal*      up 4-00:00:00      4  down* compute-7-2-[14,16],compute-7-6-[23,25]
normal*      up 4-00:00:00     50  alloc compute-6-9-[2-39],compute-7-2-[0-2,10-13,15,17-20]
normal*      up 4-00:00:00     40   idle compute-7-2-[3-9,21-39],compute-7-3-[32-39],compute-7-6-[24,26-30]
</pre></div>
</div>
<p>In the example above, there are 96 nodes in the cluster.  The different states describe the nodes.  The nodes that are <code class="docutils literal notranslate"><span class="pre">alloc</span></code> are currently running tasks and are unavailable at this time.  The nodes that are <code class="docutils literal notranslate"><span class="pre">idle</span></code> are available to be used and the nodes that are <code class="docutils literal notranslate"><span class="pre">down</span></code> are down for service or because of an error.  Currently, if the user were to schedule a task, the idle nodes would be used first to process the queued task.  If all of the nodes are allocated, then Slurm steps in and will queue the job and process it as resources become available.</p>
<p>It is worth noting there are currently SRdebugnodenum debug nodes that allow the user to compile software and to test out the code that is to be run on the main worker nodes.  This means that the user can queue a process knowing that it will run instead of waiting for the queue to run their process only to find it does not work. <strong>Debugging/ Prototying should never be done on the normal nodes</strong>.  The following is an example of a test script that the user user should following in debugging.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> [CBnetid@CBsysname Linux]$ cat chessjob.sh
 #!/bin/bash
 #SBATCH --ntasks=1
 #SBATCH --time=00:01:00
 #SBATCH --mail-user=CBnetid@utdallas.edu
 #SBATCH --mail-type=ALL
 #SBATCH -p debug

 cd /home/CBnetid/scratch/stockfish-9-linux/Linux
 ./chessrun
[jxp180019@CBsysname Linux]$
</pre></div>
</div>
<p>Line 1 is the required bash script setup. Line 2 sets of the number of cores, which should be 1 since all users can only access SRdebugnodenum debug nodes.  The next line is the time to run, which is a minute.  The amount of time should be long enough for to ensure that the program works, but no longer.  In this case, a minute was enough to know that no errors had occured.</p>
</div>
<div class="section" id="queuing-a-task">
<h2>5.2 - Queuing a Task<a class="headerlink" href="#queuing-a-task" title="Permalink to this headline">¶</a></h2>
<p>Now that you have done the preoperative tasks, it is time to queue a task.  To queue a task with Slurm, the request should be submitted as a shell script.  A number of attributes that are Slurm directives need to be established in order to queue the job.  There are two major types of tasks, serial and parallel.</p>
<div class="section" id="serial-task">
<h3>5.2.1 - Serial Task<a class="headerlink" href="#serial-task" title="Permalink to this headline">¶</a></h3>
<p>The following is a simple serial task template for the operating script.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ cat job.serial
#!/bin/bash

#SBATCH -J test               # Job name
#SBATCH -o job.%j.out         # Name of stdout output file (%j expands to jobId)
#SBATCH -N 1                  # Total number of nodes requested
#SBATCH -n 1                  # Total number of mpi tasks requested
#SBATCH -t 01:30:00           # Run time (hh:mm:ss) - 1.5 hours

# Launch serial code

echo &quot;this is serial code&quot;
sleep 10
</pre></div>
</div>
<p>In the example above the user submitted a job named <code class="docutils literal notranslate"><span class="pre">job.serial</span></code>. It will send the user an email when the job starts and finishes. The job is submitted to <em>1 compute node</em> and asked for <em>1 core</em> in that node. If the user requires more cores, more nodes may be used (ex: user requests 35 cores &#64; 16 cores per node, they get 3 nodes.)  The output of the program will go to <code class="docutils literal notranslate"><span class="pre">job.&lt;JobID&gt;.out</span></code> file. While this choice is arbitrary, if the user intends to export these to Windows, the best file output would be .txt . When the user submits the job, this file will be created for the user in their home directory. The choose partition <code class="docutils literal notranslate"><span class="pre">normal</span></code> and the user’s account is <code class="docutils literal notranslate"><span class="pre">jxw150830</span></code>. Note that the <strong>partition name is case sensitive</strong>. The optional command <code class="docutils literal notranslate"><span class="pre">sleep</span> <span class="pre">10</span></code> is used just for example purposes. It says wait another 10 seconds before ending the job.  For a complete listing of slurm commands, see Appendix B - Slurm Commands.</p>
</div>
<div class="section" id="parallel-tasks">
<h3>5.2.2 - Parallel Tasks<a class="headerlink" href="#parallel-tasks" title="Permalink to this headline">¶</a></h3>
<p>Parallel tasks use mpi technology to run multiple tasks at a time.  The script to submit an MPI script is similar to the serial, but there are some differences.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ cat job.mpi
#!/bin/bash

#SBATCH -J test               # Job name
#SBATCH -o job.%j.out         # Name of stdout output file (%j expands to jobId)
#SBATCH -N 2                  # Total number of nodes requested
#SBATCH -n 16                 # Total number of mpi tasks requested
#SBATCH -t 01:30:00           # Run time (hh:mm:ss) - 1.5 hours

# Launch MPI-based executable

prun ./a.out
</pre></div>
</div>
<p>Once the user has set up the file, the user can submit the job to the Slurm batch that will apply to the system using <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ sbatch job.serial
Submitted batch job 405
</pre></div>
</div>
<p>This informs the user of the job number.  The user will also receive an email from SRslurmemail informing that the job has started.  If the user ever forgets the job number, or has logged into SRuppersysName to determine if and which jobs are running, the user can type the command <code class="docutils literal notranslate"><span class="pre">squeue</span> <span class="pre">-u</span> <span class="pre">$USER</span></code>.  This will show all current running tasks to the user, with the first number being the Job ID.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ squeue -u $USER
  JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
    405    normal     test jxw15083  R       0:01      1 compute-7-2-21
</pre></div>
</div>
<p>If the user runs <code class="docutils literal notranslate"><span class="pre">squeue</span></code> without the additional command, the queue total queue for the cluster will be displayed.</p>
</div>
</div>
<div class="section" id="checking-on-a-running-task">
<h2>5.3 - Checking on a Running Task<a class="headerlink" href="#checking-on-a-running-task" title="Permalink to this headline">¶</a></h2>
<p>Once the task is running, the user may want to check on the progress of the task.  This can be done by using the command <code class="docutils literal notranslate"><span class="pre">sstat</span> <span class="pre">--format=AveCPU,AvePages,AveRSS,AveVMSize,JobID</span> <span class="pre">-j</span> <span class="pre">&lt;JobID&gt;</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ sstat --format=AveCPU,AvePages,AveRSS,AveVMSize,JobID -j 405
    AveCPU   AvePages     AveRSS  AveVMSize        JobID
---------- ---------- ---------- ---------- ------------
 00:00.000          0       362K      4372K 405.0
</pre></div>
</div>
<p>For those interested in very detailed analysis, running the command <code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">show</span> <span class="pre">job</span> <span class="pre">--d</span> <span class="pre">&lt;JobID&gt;</span></code> with the job number will give the user a listing that is very specific about how the job is being executed.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ scontrol show job --d 405
JobId=405 JobName=test
   UserId=jxw150830(532471) GroupId=oithpc(1100) MCS_label=N/A
   Priority=4294901737 Nice=0 Account=(null) QOS=(null)
   JobState=COMPLETED Reason=None Dependency=(null)
   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   DerivedExitCode=0:0
   RunTime=00:00:10 TimeLimit=01:30:00 TimeMin=N/A
   SubmitTime=2018-05-31T16:06:29 EligibleTime=2018-05-31T16:06:29
   StartTime=2018-05-31T16:06:29 EndTime=2018-05-31T16:06:39 Deadline=N/A
   PreemptTime=None SuspendTime=None SecsPreSuspend=0
   LastSchedEval=2018-05-31T16:06:29
   Partition=normal AllocNode:Sid=CBsysname:449124
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=compute-7-2-21
   BatchHost=compute-7-2-21
   NumNodes=1 NumCPUs=16 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   TRES=cpu=16,node=1,billing=16
   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
   Nodes=compute-7-2-21 CPU_IDs=0-15 Mem=0 GRES_IDX=
   MinCPUsNode=1 MinMemoryNode=0 MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   Gres=(null) Reservation=(null)
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/home/jxw150830/job.serial
   WorkDir=/home/jxw150830
   StdErr=/home/jxw150830/job.405.out
   StdIn=/dev/null
   StdOut=/home/jxw150830/job.405.out
   Power=
</pre></div>
</div>
<p>If at any time the user wants to cancel a job, the user should execute <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">&lt;JobID&gt;</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ sbatch sampletask.sh
Submitted batch job 380
[CBnetid ~]$ scancel 380
[CBnetid ~]$
</pre></div>
</div>
<p>If there are no issues, there will be a clean output in the terminal and the job will disappear from the queue.</p>
</div>
<div class="section" id="debugging-mpi-with-slurm-and-gdb">
<h2>5.4 - Debugging MPI with Slurm and gdb<a class="headerlink" href="#debugging-mpi-with-slurm-and-gdb" title="Permalink to this headline">¶</a></h2>
<p>When the user is running a MPI task, it is important to debug properly.  Because the amount of data that can be dumped is greater than the quota for the home directory, special care must be taken in order for the user to sucessfully fix code.  The user needs to first ssh into SRuppersysName using the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>jwhite-swift@hpc-rca:~$ ssh -X jxw150830@CBsysname.utdallas.edu
jxw150830@CBsysname.utdallas.edu&#39;s password:
Last login: Tue Jun  5 10:26:09 2018 from 10.21.4.24
Disk quotas for user jxw150830:
====================    ============    ================        ============
Disk                    Usage           Soft Limit              Hard Limit
====================    ============    ================        ============
/home/jxw150830         8248K           20000M                  30000M
====================    ============    ================        ============
[CBnetid ~]$
</pre></div>
</div>
<p>Note that the command is <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">-X</span> <span class="pre">&lt;NetID&gt;&#64;CBsysname.utdallas.edu</span></code>.  The <code class="docutils literal notranslate"><span class="pre">-X</span></code> allows the user to pass visual windows back through, which will become important later.</p>
<p>Once logged into SRuppersysName, the user must first get a reservation on a <strong>debug</strong> compute node.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ salloc -p debug -N1 -n4 --time=00:30:00
salloc: Granted job allocation 620
Disk quotas for user jxw150830:
====================    ============    ================        ============
Disk                    Usage           Soft Limit              Hard Limit
====================    ============    ================        ============
/home/jxw150830         8248K           20000M                  30000M
====================    ============    ================        ============
[CBnetid ~]$
</pre></div>
</div>
<p>This command asks for 1 node and 4 cores on the node in the debug partition for 30 minutes.  The number of cores can be adjusted as required.</p>
<p>To find out the current user’s node, the user then inputs <code class="docutils literal notranslate"><span class="pre">showq</span> <span class="pre">–u</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ squeue -u $USER
           JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
             620     debug     bash jxw15083  R       4:55      1 CNChapter5.4
</pre></div>
</div>
<p>Then the user will ssh into that node and turn on X forwarding, just as the user did when accessing the SRuppersysName node to begin with.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ ssh -X CNChapter5.4
Warning: Permanently added &#39;CNChapter5.4,10.182.224.70&#39; (ECDSA) to the list of known hosts.
[jxw150830@CNChapter5.4 ~]$
</pre></div>
</div>
<p>Now that the user is on the debug node, the user needs to run the following command: <code class="docutils literal notranslate"><span class="pre">mpirun</span> <span class="pre">-np</span> <span class="pre">4</span> <span class="pre">xterm</span> <span class="pre">-e</span> <span class="pre">gdb</span> <span class="pre">&lt;my_mpi_application&gt;</span></code></p>
<img alt="BaseRSTs\./assets/5.4.png" src="BaseRSTs\./assets/5.4.png" />
<p>This will then produce 4 screens (or the number specified by <code class="docutils literal notranslate"><span class="pre">np</span></code>) that allow the user to debug each of the instances that are running using MPI, without having to core dump extremely large files</p>
<p>In order for this to work, the user needs to be running some form of an X server locally. If the user is on a linux machine, this functionality will be out of the box. If you are on a Mac, you’ll need XQuartz. If you are on a windows machine, you should use MobaXterm.  These programs are discribed in Section 1 - Connecting to SRuppersysName.</p>
</div>
<div class="section" id="running-interactive-jobs">
<h2>5.5 - Running Interactive Jobs<a class="headerlink" href="#running-interactive-jobs" title="Permalink to this headline">¶</a></h2>
<p>Interactive Jobs can be run by the user on the individual compute nodes.  This is done by running a slurm command that places the user onto a compute node or nodes.  This then allows a user to run commands on the compute nodes.</p>
<div class="section" id="running-one-time-instance-jobs">
<h3>5.5.1 - Running One-Time Instance Jobs<a class="headerlink" href="#running-one-time-instance-jobs" title="Permalink to this headline">¶</a></h3>
<p>If the user is interested in only running for the instance in the compute node (i.e. logged off when the session is finished, then the user should use <code class="docutils literal notranslate"><span class="pre">srun</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ srun -n1 -N1 --pty /bin/bash
[CBuserCompute ~]$
</pre></div>
</div>
<p>The user is now logged into the compute node.  Notice that the <code class="docutils literal notranslate"><span class="pre">-n1</span></code> denotes 1 task (or CPU) and the <code class="docutils literal notranslate"><span class="pre">-N1</span></code> denotes 1 node is being used.  The rest of the command sets up the machine to be interacted with. If the node is currently in use, the user will be placed in a queue and the command will hang until the user is granted access.  Once the user has finished using the resources, the user needs to type <code class="docutils literal notranslate"><span class="pre">exit</span></code> to exit the compute node.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[[CBuserCompute] ~]$ exit
exit
[CBnetid ~]$
</pre></div>
</div>
<p>Once executed, the session is closed and the user moves back to the SRuppersysName node.</p>
</div>
<div class="section" id="running-persistent-jobs">
<h3>5.5.2 - Running Persistent Jobs<a class="headerlink" href="#running-persistent-jobs" title="Permalink to this headline">¶</a></h3>
<p>If the user is interested in running a task that can needs to be logged in and out of multiple times, the user should allocate some time on a node or nodes using <code class="docutils literal notranslate"><span class="pre">salloc</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ salloc -n1 -N1 -t 1:00:00
salloc: Granted job allocation 607
Disk quotas for user jxw150830:
====================    ============    ================        ============
Disk                    Usage           Soft Limit              Hard Limit
====================    ============    ================        ============
/home/jxw150830         7420K           20000M                  30000M
====================    ============    ================        ============
[CBnetid ~]$
</pre></div>
</div>
<p>The user in this example allocated <code class="docutils literal notranslate"><span class="pre">-n1</span></code> for 1 processor and <code class="docutils literal notranslate"><span class="pre">-N1</span></code> for 1 node.  The <code class="docutils literal notranslate"><span class="pre">-t</span> <span class="pre">1:00:00</span></code> sets the allocation to 1 hour.  Now that the user has allocated the node, the user needs to find the compute node number by <code class="docutils literal notranslate"><span class="pre">running</span> <span class="pre">squeue</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ squeue -u $USER
           JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
             607    normal     bash jxw15083  R       3:46      1 CNChapter5.5.5
</pre></div>
</div>
<p>Now that the compute node number is known, the user can ssh into the node to work. Note: users can only ssh into nodes that have been allocated for them.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ ssh CNChapter5.5.5
Warning: Permanently added &#39;CNChapter5.5.5,10.182.224.72&#39; (ECDSA) to the list of known hosts.
[jxw150830@CNChapter5.5.5 ~]$
</pre></div>
</div>
<p>Now that the user is in the node, the user is free to come and go to do work for the duration of the allocation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[jxw150830@CNChapter5.5.5 ~]$ exit
logout
Connection to CNChapter5.5.5 closed.
[CBnetid ~]$ ssh CNChapter5.5.5
[jxw150830@CNChapter5.5.5 ~]$
</pre></div>
</div>
</div>
<div class="section" id="logging-into-a-particular-node">
<h3>5.5.3 - Logging into a Particular Node<a class="headerlink" href="#logging-into-a-particular-node" title="Permalink to this headline">¶</a></h3>
<p>There are times when it is advantageous for the user to work on a particular node.  This may be for a particular scipt or some other program that is loaded on a particular set of nodes, or to use the particular node because of hardware.  To do this, the user must us the command <code class="docutils literal notranslate"><span class="pre">salloc</span> <span class="pre">-w</span> <span class="pre">&lt;computenode&gt;</span></code>.  If the node is free, the user will encounter the following output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ salloc -w CNChapter5.5.3
salloc: Granted job allocation 611
Disk quotas for user CBnetid:
====================    ============    ================        ============
Disk                    Usage           Soft Limit              Hard Limit
====================    ============    ================        ============
/home/jxw150830         7424K           20000M                  30000M
====================    ============    ================        ============
[CBnetid ~]$ ssh CNChapter5.5.3
Warning: Permanently added &#39;CNChapter5.5.3,10.182.224.204&#39; (ECDSA) to the list of known hosts.
[CBnetid@CNChapter5.5.3 ~]$
</pre></div>
</div>
<p>and if the node is in use, the user will see:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ salloc -w compute-6-9-3
salloc: Pending job allocation 608
salloc: job 608 queued and waiting for resources
</pre></div>
</div>
<p>The user will then have to wait until the node becomes available.</p>
</div>
</div>
<div class="section" id="viewing-results">
<h2>5.6 - Viewing Results<a class="headerlink" href="#viewing-results" title="Permalink to this headline">¶</a></h2>
<p>Once the job is done, the user will receive an email from SRslurmemail alerting the user that the job has been completed. Any interaction between the user and the nodes that the user were logged into will be be closed and the user will be returned to the home folder.  The file will be in that directory.  If there are any errors that occur, those will be captured in the output file that is created by the system.  This is especially useful when running on the debugging nodes.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[CBnetid ~]$ ls
html  job.405.out  job.mpi  job.serial  sampletask.sh  scratch
[CBnetid ~]$ cat job.405.out
this is serial code
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Noah Castetter

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>